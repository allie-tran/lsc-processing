{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import bamboolib as bam\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "import json\n",
    "import geopy.distance\n",
    "from nltk import ngrams\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unhelpful_images = json.load(open(\"files/unhelpful_images.json\"))\n",
    "metadata = pd.read_csv('VAISL/files/final_metadata.csv', sep=',', decimal='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['checkin'] = metadata['checkin'].fillna(\"\")\n",
    "metadata['city'] = metadata['city'].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove = [\"Province\", \"Área metropolitana de Madrid y Corredor del Henares\", \"Community of\", \"The Municipal District of\",\n",
    "                  \"Kreis\", \"Landkreis\", \"Regional Unit\", \"Municipal Unit\", \"Municipality\", \"Administrative District\", \"Region of\",\n",
    "                  \"Provence-Alpes-Côte d'Azur\", \"Municipal Borough District\", \"Subdistrict Administrative Organization\", \"Subdistrict\",\n",
    "                  \"District\", \"Distretto di\", \"Municipal District\", \"City\", \"Land \", \"Urban agglomeration\"]\n",
    "words_to_remove = sorted(words_to_remove, key=lambda x: -len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra(city):\n",
    "    city = city.split(\",\")\n",
    "    new_city = []\n",
    "    for name in city:\n",
    "        for word in words_to_remove:\n",
    "            name = name.replace(word, \"\")\n",
    "        name = name.replace(\"of \", \"\")\n",
    "        name = name.strip()\n",
    "        if name and name not in new_city:\n",
    "            new_city.append(name)\n",
    "    return \", \".join(new_city)\n",
    "        \n",
    "metadata[\"city\"] = metadata[\"city\"].apply(remove_extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New timezone processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geojson\n",
    "country_geojson = geojson.load(open(\"files/countries.geojson\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_countries = set(metadata[\"country\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "geojson_data = {}\n",
    "for country in country_geojson[\"features\"]:\n",
    "    name = country[\"properties\"][\"ADMIN\"]\n",
    "    if name in all_countries or name in [\"United Kingdom\", \"South Korea\"]:\n",
    "        geojson_data[name] = country\n",
    "geojson_data[\"Korea\"] = geojson_data[\"South Korea\"]\n",
    "geojson_data[\"England\"] = geojson_data[\"United Kingdom\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(geojson_data, open(\"files/backend/countries.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[\"new_timezone\"] = metadata[\"new_timezone\"].ffill()\n",
    "metadata[\"country\"] = metadata[\"country\"].fillna(\"\")\n",
    "metadata[\"OCR\"] = metadata[\"OCR\"].fillna(\"\")\n",
    "metadata[\"location_info\"] = metadata.apply(lambda row: row[\"categories\"] if row[\"stop\"] else row[\"checkin\"], axis=1)\n",
    "metadata[\"location_info\"] = metadata[\"location_info\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad993268439548c4b58e4e56f7cc8910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/723329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "info_dict = {}\n",
    "def to_full_key(image):\n",
    "    return f\"{image[:6]}/{image[6:8]}/{image}\"\n",
    "\n",
    "def to_local_time(utc_time, time_zone):\n",
    "    return utc_time.astimezone(tz.gettz(time_zone))\n",
    "\n",
    "# Calculate seconds from midnight from a datetime object\n",
    "def seconds_from_midnight(time):\n",
    "    return time.hour * 3600 + time.minute * 60 + time.second\n",
    "\n",
    "for index, row in tqdm(metadata.iterrows(), total=len(metadata)):\n",
    "    image = row['ImageID']\n",
    "    if isinstance(image, str):\n",
    "        if image not in unhelpful_images:\n",
    "            image = to_full_key(image)\n",
    "            utc_time = datetime.strptime(row[\"minute_id\"]+\"00\", \"%Y%m%d_%H%M%S\").replace(tzinfo=tz.gettz('UTC'))\n",
    "            local_time = to_local_time(utc_time, row[\"new_timezone\"])\n",
    "            info_dict[image] = {\n",
    "                \"image_path\": image,\n",
    "                \"minute_id\": row[\"minute_id\"],\n",
    "                \"time\": datetime.strftime(local_time, \"%Y/%m/%d %H:%M:00%z\"),\n",
    "                \"utc_time\": datetime.strftime(utc_time, \"%Y/%m/%d %H:%M:00%z\"),\n",
    "                \"weekday\": datetime.strftime(local_time, \"%A\").lower(),\n",
    "                \"descriptions\": row['Tags'].lower().split(',') if isinstance(row['Tags'], str) else \"\",\n",
    "                \"address\": row[\"city\"],\n",
    "                \"location\": row[\"checkin\"] if row[\"stop\"] else \"None\",\n",
    "                \"location_info\": row[\"location_info\"],\n",
    "                \"gps\": {\"lat\": row[\"new_lat\"],\n",
    "                        \"lon\": row[\"new_lng\"]},\n",
    "                \"region\": row[\"city\"].lower().split(', '),\n",
    "                \"country\": row[\"country\"].lower(),\n",
    "                \"ocr\": row[\"OCR\"].split(', '),\n",
    "                \"timestamp\": utc_time.timestamp(),\n",
    "                \"seconds_from_midnight\": seconds_from_midnight(local_time)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "fields_to_fix = [\"address\", \"location\", \"region\"]\n",
    "for image in info_dict:\n",
    "    for field in fields_to_fix:\n",
    "        if isinstance(info_dict[image][field], str):\n",
    "            info_dict[image][field] = unidecode(\n",
    "                info_dict[image][field])\n",
    "        elif isinstance(info_dict[image][field], list):\n",
    "            info_dict[image][field] = [unidecode(s) for s in info_dict[image][field]]\n",
    "        elif np.isnan(info_dict[image][field]):\n",
    "            info_dict[image][field] = \"NONE\"\n",
    "        else:\n",
    "            print(field, info_dict[image][field])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_path': '201902/08/20190208_172845_000.jpg',\n",
       " 'minute_id': '20190208_1728',\n",
       " 'time': '2019/02/08 20:28:00+0300',\n",
       " 'utc_time': '2019/02/08 17:28:00+0000',\n",
       " 'weekday': 'friday',\n",
       " 'descriptions': ['text', 'person', 'indoor', 'store'],\n",
       " 'address': 'Turkey, Marmara',\n",
       " 'location': 'Istanbul Ataturk Airport',\n",
       " 'location_info': 'Airport',\n",
       " 'gps': {'lat': 40.984292, 'lon': 28.8156077},\n",
       " 'region': ['turkey', 'marmara'],\n",
       " 'country': 'turkey',\n",
       " 'ocr': ['gặp,lại,ain,TRÀ,CHANH,CROS,KHUY,Chupa,Chúps,TMINT,DOUDLEMIN,ININTIN'],\n",
       " 'timestamp': 1549646880.0,\n",
       " 'ocr_score': {}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_dict[\"201901/06/20190106_171105_000.jpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714583 714583\n"
     ]
    }
   ],
   "source": [
    "groups = json.load(open('files/group_segments.json'))\n",
    "\n",
    "assigned = []\n",
    "count = 0\n",
    "for group_name in groups:\n",
    "    group_id = int(group_name.split('_')[-1])\n",
    "    before = []\n",
    "    if group_id > 1:\n",
    "        before_group_id = group_id - 1\n",
    "        while before_group_id >= 1:\n",
    "            before_group = groups[f\"G_{before_group_id}\"]\n",
    "            if before_group[\"location\"] != \"NONE\":\n",
    "                for scene in before_group[\"scenes\"]:\n",
    "                    before.extend(scene[1])\n",
    "                break\n",
    "            before_group_id -= 1\n",
    "            \n",
    "    after = []\n",
    "    if group_id < len(groups):\n",
    "        after_group_id = group_id + 1\n",
    "        while after_group_id  <= len(groups) - 1:\n",
    "            after_group = groups[f\"G_{after_group_id}\"]\n",
    "            if after_group[\"location\"] != \"NONE\":\n",
    "                for scene in after_group[\"scenes\"]:\n",
    "                    after.extend(scene[1])\n",
    "                break\n",
    "            after_group_id +=1\n",
    "    for scene_name, images in groups[group_name][\"scenes\"]:\n",
    "        for image in images:\n",
    "            if image in info_dict:\n",
    "                info_dict[image][\"scene\"] = scene_name\n",
    "                info_dict[image][\"group\"] = group_name\n",
    "                info_dict[image][\"before\"] = before[:10]\n",
    "                info_dict[image][\"after\"] = after[:10]\n",
    "                count += 1\n",
    "                assigned.append(image)\n",
    "            else:\n",
    "                print(\"Skipping\", image)\n",
    "\n",
    "print(len(set(assigned)), len(info_dict))\n",
    "# I THINK THERE'S SOMETHING WRONG HERE\n",
    "if len(set(assigned)) < len(info_dict):\n",
    "    to_remove = set(info_dict.keys()).difference(assigned)\n",
    "    for img in to_remove:\n",
    "        del info_dict[img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_path': '201901/01/20190101_164846_000.jpg',\n",
       " 'minute_id': '20190101_1648',\n",
       " 'time': '2019/01/01 16:48:00+0000',\n",
       " 'utc_time': '2019/01/01 16:48:00+0000',\n",
       " 'weekday': 'tuesday',\n",
       " 'descriptions': ['person',\n",
       "  'food',\n",
       "  'table',\n",
       "  'plate',\n",
       "  'indoor',\n",
       "  'eating',\n",
       "  'dessert',\n",
       "  'meal'],\n",
       " 'address': 'Dublin, Ireland, Leinster',\n",
       " 'location': \"Eddie Rocket's\",\n",
       " 'location_info': 'Burger Joint, Diner, Fast Food Restaurant',\n",
       " 'gps': {'lat': 53.2828644, 'lon': -6.4222863},\n",
       " 'region': ['dublin', 'ireland', 'leinster'],\n",
       " 'country': 'ireland',\n",
       " 'ocr': '',\n",
       " 'timestamp': 1546361280.0,\n",
       " 'ocr_score': {},\n",
       " 'scene': 'S_250',\n",
       " 'group': 'G_17',\n",
       " 'before': ['201901/01/20190101_154407_000.jpg',\n",
       "  '201901/01/20190101_154439_000.jpg',\n",
       "  '201901/01/20190101_154511_000.jpg',\n",
       "  '201901/01/20190101_154543_000.jpg',\n",
       "  '201901/01/20190101_154615_000.jpg',\n",
       "  '201901/01/20190101_154647_000.jpg',\n",
       "  '201901/01/20190101_154719_000.jpg',\n",
       "  '201901/01/20190101_154751_000.jpg',\n",
       "  '201901/01/20190101_154823_000.jpg',\n",
       "  '201901/01/20190101_154855_000.jpg'],\n",
       " 'after': ['201901/01/20190101_170843_000.jpg',\n",
       "  '201901/01/20190101_170915_000.jpg',\n",
       "  '201901/01/20190101_170947_000.jpg',\n",
       "  '201901/01/20190101_171019_000.jpg',\n",
       "  '201901/01/20190101_171123_000.jpg',\n",
       "  '201901/01/20190101_171401_000.jpg',\n",
       "  '201901/01/20190101_171433_000.jpg',\n",
       "  '201901/01/20190101_171537_000.jpg',\n",
       "  '201901/01/20190101_171815_000.jpg']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_dict[\"201901/01/20190101_164846_000.jpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(info_dict, open(f\"files/info_dict.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARE BACKEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "719\n"
     ]
    }
   ],
   "source": [
    "locations = set([img[\"location\"].lower().strip() for img in info_dict.values()])\n",
    "if \"none\" in locations:\n",
    "    locations.remove(\"none\")\n",
    "extra = set()\n",
    "location_with_extras = {}\n",
    "for loc in locations:\n",
    "    if loc:\n",
    "        location_with_extras[loc] = []\n",
    "        for lengram in range(2, len(loc)):\n",
    "            for ngram in ngrams(loc.split(), lengram):\n",
    "                location_with_extras[loc].append(\" \".join(ngram))\n",
    "        location_with_extras[loc].append(loc)\n",
    "        location_with_extras[loc] = location_with_extras[loc][::-1]\n",
    "json.dump(location_with_extras, open(f'files/backend/locations.json', 'w'))\n",
    "print(len(locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = set([loc.lower().strip() for img in info_dict.values()\n",
    "               for loc in img[\"region\"]])\n",
    "json.dump(list(regions), open(f'files/backend/regions.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../UI/src/regions.js\", 'w') as f:\n",
    "    f.write(\"var regions=\" + json.dumps(list(regions)) + \";\\n\\nexport default regions;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dict(image):\n",
    "    return { key: info_dict[image][key] for key in [\"group\", \"scene\", \"time\", \"gps\", \"location\", \"location_info\"]}\n",
    "\n",
    "basic_dict = {image: filter_dict(image) for image in info_dict}\n",
    "json.dump(basic_dict, open(f'files/backend/basic_dict.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_info = {}\n",
    "def get_hour_minute(date_string):\n",
    "    datetime_value = datetime.strptime(date_string, \"%Y/%m/%d %H:%M:00%z\")\n",
    "    return datetime_value.strftime(\"%I:%M%p\")\n",
    "\n",
    "def get_final_time(first_info, last_info):\n",
    "    if first_info == last_info:\n",
    "        return first_info\n",
    "    return f\"{first_info} - {last_info}\"\n",
    "\n",
    "for group_name in groups:\n",
    "    group_first_info = None\n",
    "    group_last_info = None\n",
    "    for scene_name, images in groups[group_name][\"scenes\"]:\n",
    "        first_info = info_dict[images[0]][\"time\"]\n",
    "        last_info = info_dict[images[-1]][\"time\"]\n",
    "        if not group_first_info:\n",
    "            group_first_info = first_info\n",
    "        group_last_info = last_info\n",
    "        time_info[scene_name] = get_final_time(get_hour_minute(first_info), get_hour_minute(last_info))\n",
    "    time_info[group_name] = get_final_time(get_hour_minute(group_first_info), get_hour_minute(group_last_info))\n",
    "\n",
    "json.dump(time_info, open(f\"files/backend/time_info.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11:02AM'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_info[\"S_36\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
