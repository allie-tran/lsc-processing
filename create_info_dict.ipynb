{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import bamboolib as bam\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "import json\n",
    "import geopy.distance\n",
    "from nltk import ngrams\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_for_ocr(word):\n",
    "    final_text = defaultdict(float)\n",
    "    final_text[word] = 1\n",
    "    for i in range(0, len(word)-1):\n",
    "        if len(word[:i+1]) > 1:\n",
    "            final_text[word[:i+1]] += (i+1) / len(word)\n",
    "        if len(word[i+1:]) > 1:\n",
    "            final_text[word[i+1:]] += 1 - (i+1)/len(word)\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ocr_scores = json.load(open(\"../original_data/OCR_201901.json\"))\n",
    "ocr_scores = {}\n",
    "# for year in os.listdir(\"/home/nmduy/LSC2022/LSC_Metada/OCR/text_area/\"):\n",
    "#     if \".json\" in year:\n",
    "#         ocr_scores.update(json.load(open(f\"/home/nmduy/LSC2022/LSC_Metada/OCR/text_area/{year}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unhelpful_images = json.load(open(\"files/unhelpful_images.json\"))\n",
    "# metadata = pd.read_csv('files/processed.csv', sep=',', decimal='.')\n",
    "metadata = pd.read_csv('VAISL/files/final_metadata.csv', sep=',', decimal='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Old timezone processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Europe/Dublin          150918\n",
       "Asia/Bangkok             7504\n",
       "Europe/Athens            2042\n",
       "Asia/Ho_Chi_Minh         1763\n",
       "Europe/Madrid            1494\n",
       "Europe/Istanbul          1156\n",
       "Europe/London            1017\n",
       "Asia/Seoul                938\n",
       "Europe/Paris              588\n",
       "Europe/Berlin             502\n",
       "Europe/Zurich             399\n",
       "Australia/Melbourne       383\n",
       "Europe/Copenhagen         220\n",
       "Europe/Bucharest          131\n",
       "Europe/Oslo               127\n",
       "America/Toronto            83\n",
       "Etc/GMT-6                  67\n",
       "Etc/GMT                    62\n",
       "Europe/Sofia               55\n",
       "Etc/GMT-8                  36\n",
       "Europe/Belgrade            10\n",
       "Europe/Amsterdam            9\n",
       "Asia/Phnom_Penh             7\n",
       "Europe/Zagreb               6\n",
       "Asia/Kolkata                5\n",
       "Europe/Budapest             5\n",
       "Europe/Brussels             3\n",
       "Europe/Ljubljana            2\n",
       "Europe/Vienna               2\n",
       "Etc/GMT-2                   2\n",
       "Name: time_zone, dtype: Int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "metadata['time_zone'] = metadata['time_zone'].astype('string')\n",
    "metadata['time_zone'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 100\n",
    "timezones = metadata['time_zone'].value_counts()\n",
    "cutoff = len(timezones)\n",
    "for i in range(len(timezones)):\n",
    "    if timezones[i] < threshold:\n",
    "        cutoff = i\n",
    "        break\n",
    "okay_timezones = timezones.index.values[:cutoff].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_condition = metadata['time_zone'].isin(okay_timezones)\n",
    "metadata.loc[tmp_condition, 'time_zone'] = metadata['time_zone']\n",
    "metadata.loc[~tmp_condition, 'time_zone'] = np.nan\n",
    "metadata[['time_zone']] = metadata[['time_zone']].fillna(method='ffill')\n",
    "metadata[['time_zone']] = metadata[['time_zone']].fillna(method='bfill')\n",
    "metadata['ImageID'] = metadata['ImageID'].astype('string')\n",
    "# metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_zones = {\"Europe/Dublin\": \"ireland\",\n",
    "              \"Europe/Athens\": \"greece\",\n",
    "              \"Europe/Berlin\": \"germany\",\n",
    "              \"Asia/Bangkok\": \"thailand\",\n",
    "              \"Asia/Ho_Chi_Minh\": \"vietnam\",\n",
    "              \"Europe/Madrid\": \"spain\",\n",
    "              \"Europe/Istanbul\": \"turkey\",\n",
    "              \"Europe/London\": \"england\",\n",
    "              \"Asia/Seoul\": \"korea\",\n",
    "              \"Europe/Paris\": \"france\",\n",
    "              \"Europe/Zurich\": \"switzerland\",\n",
    "              \"Australia/Melbourne\": \"australia\",\n",
    "              \"Europe/Copenhagen\": \"denmark\",\n",
    "              \"Europe/Bucharest\": \"romania\",\n",
    "              \"Europe/Oslo\": \"norway\"}\n",
    "all_countries = time_zones.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New timezone processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geojson\n",
    "country_geojson = geojson.load(open(\"../original_data/countries.geojson\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_countries = set(metadata[\"country\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "geojson_data = {}\n",
    "for country in country_geojson[\"features\"]:\n",
    "    name = country[\"properties\"][\"ADMIN\"]\n",
    "    if name.lower() in all_countries or name in [\"United Kingdom\", \"South Korea\"]:\n",
    "        geojson_data[name] = country\n",
    "geojson_data[\"Korea\"] = geojson_data[\"South Korea\"]\n",
    "geojson_data[\"England\"] = geojson_data[\"United Kingdom\"]\n",
    "\n",
    "with open(f\"/home/tlduyen/LSC2020/LSC2020/ui/src/worldmap.js\", 'w') as f:\n",
    "    f.write(\"var worldmap=\" + json.dumps(geojson_data) + \";\\n\\nexport {worldmap};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(geojson_data, open(\"files/backend/countries.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# def create(ocr_scores):\n",
    "#     tf = {}\n",
    "#     idf = defaultdict(lambda: 0)\n",
    "#     for image, scores in tqdm(ocr_scores.items()):\n",
    "#         tf[image] = defaultdict(float)\n",
    "#         word_set = set()\n",
    "#         for score in scores:\n",
    "#             word = score['text'].lower()\n",
    "#             for subword in word.lower().split():\n",
    "#                 if len(subword) > 1:\n",
    "#                     splited_words = process_for_ocr(subword)\n",
    "#                     for w, s in splited_words.items():\n",
    "#                         if w not in stop_words:\n",
    "#                             word_set.add(w)\n",
    "#                             tf[image][w] += np.log(1 + score['area'] * 5000 * s)\n",
    "#         for word in word_set:\n",
    "#             idf[word] += 1\n",
    "\n",
    "#     tf_idf = {}\n",
    "#     print(len(tf))\n",
    "#     for image in tqdm(tf):\n",
    "#         tf_idf[image] = {}\n",
    "#         for word in tf[image]:\n",
    "#             if idf[word]:\n",
    "#                 tf_idf[image][word] = tf[image][word] * np.log(len(tf) / idf[word])\n",
    "#                 assert (tf_idf[image][word] >= 0), f\"negative value {tf_idf[image][word]}, {tf[image][word]}, {word}, {idf[word]}, {np.log(len(tf) / idf[word])}\"\n",
    "#             else:\n",
    "#                 tf_idf[image][word] = 0\n",
    "#     return idf, tf_idf\n",
    "\n",
    "# idf, tf_idf = create(ocr_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idf = dict(idf.items())\n",
    "# joblib.dump((tf_idf, idf), \"files/ocr_tfidf.joblib\")\n",
    "# tf_idf, idf = joblib.load(\"files/ocr_tfidf.joblib\")\n",
    "tf_idf, idf = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[\"new_timezone\"] = metadata[\"new_timezone\"].ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ddbfeefbbc48c489711c28c3b579b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/723329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "info_dict = {}\n",
    "\n",
    "def to_local_time(utc_time, time_zone):\n",
    "    return utc_time.astimezone(tz.gettz(time_zone))\n",
    "\n",
    "def to_full_key(image):\n",
    "    return f\"{image[:6]}/{image[6:8]}/{image}\"\n",
    "\n",
    "for index, row in tqdm(metadata.iterrows(), total=len(metadata)):\n",
    "    image = row['ImageID']\n",
    "    if isinstance(image, str):\n",
    "        if image not in unhelpful_images:\n",
    "            utc_time = datetime.strptime(row[\"minute_id\"]+\"00\", \"%Y%m%d_%H%M%S\").replace(tzinfo=tz.gettz('UTC'))\n",
    "            local_time = to_local_time(utc_time, row[\"new_timezone\"])\n",
    "            #TODO!\n",
    "            image = to_full_key(image)\n",
    "            info_dict[image] = {\n",
    "                \"image_path\": image,\n",
    "                \"minute_id\": row[\"minute_id\"],\n",
    "                \"time\": datetime.strftime(local_time, \"%Y/%m/%d %H:%M:00%z\"),\n",
    "                \"utc_time\": datetime.strftime(utc_time, \"%Y/%m/%d %H:%M:00%z\"),\n",
    "                \"weekday\": datetime.strftime(local_time, \"%A\").lower(),\n",
    "                \"descriptions\": row['Tags'].lower().split(',') if isinstance(row['Tags'], str) else \"\",\n",
    "                \"address\": row[\"city\"],\n",
    "                \"location\": row[\"new_name\"],\n",
    "                \"gps\": {\"lat\": row[\"new_lat\"],\n",
    "                        \"lon\": row[\"new_long\"]},\n",
    "                \"region\": row[\"city\"].lower().split(', ') if isinstance(row[\"city\"], str) else [],\n",
    "                \"country\": row[\"country\"].lower() if isinstance(row[\"country\"], str) else None,\n",
    "                \"ocr\": row[\"OCR\"].split(', ') if isinstance(row['OCR'], str) else \"\",\n",
    "                \"timestamp\": utc_time.timestamp() #!TODO in es.py\n",
    "            }\n",
    "\n",
    "            if image in tf_idf:\n",
    "                info_dict[image][\"ocr_score\"] = dict([item for item in tf_idf[image].items() if item[1] > 0])\n",
    "            else:\n",
    "                info_dict[image][\"ocr_score\"] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_path': '201902/08/20190208_172845_000.jpg',\n",
       " 'minute_id': '20190208_1728',\n",
       " 'time': '2019/02/08 20:28:00+0300',\n",
       " 'utc_time': '2019/02/08 17:28:00+0000',\n",
       " 'weekday': 'friday',\n",
       " 'descriptions': ['text', 'person', 'indoor', 'store'],\n",
       " 'address': 'Turkey, Marmara',\n",
       " 'location': 'Istanbul Ataturk Havalimani',\n",
       " 'gps': {'lat': 40.984292, 'lon': 28.8156077},\n",
       " 'region': ['turkey', 'marmara'],\n",
       " 'country': 'turkey',\n",
       " 'ocr': ['gặp,lại,ain,TRÀ,CHANH,CROS,KHUY,Chupa,Chúps,TMINT,DOUDLEMIN,ININTIN'],\n",
       " 'timestamp': 1549646880.0,\n",
       " 'ocr_score': {},\n",
       " 'scene': 'S_16002',\n",
       " 'group': 'G_705',\n",
       " 'before': ['201902/08/20190208_172136_000.jpg',\n",
       "  '201902/08/20190208_172208_000.jpg',\n",
       "  '201902/08/20190208_172240_000.jpg',\n",
       "  '201902/08/20190208_172312_000.jpg',\n",
       "  '201902/08/20190208_172344_000.jpg',\n",
       "  '201902/08/20190208_172416_000.jpg',\n",
       "  '201902/08/20190208_172501_000.jpg'],\n",
       " 'after': ['201902/08/20190208_172949_000.jpg',\n",
       "  '201902/08/20190208_173021_000.jpg',\n",
       "  '201902/08/20190208_173053_000.jpg',\n",
       "  '201902/08/20190208_173125_000.jpg',\n",
       "  '201902/08/20190208_173157_000.jpg',\n",
       "  '201902/08/20190208_173229_000.jpg',\n",
       "  '201902/08/20190208_173301_000.jpg',\n",
       "  '201902/08/20190208_173333_000.jpg',\n",
       "  '201902/08/20190208_173405_000.jpg']}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_dict[\"201902/08/20190208_172845_000.jpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714583 714583\n"
     ]
    }
   ],
   "source": [
    "both = both.loc[both['ImageID'].str.contains('20190208_174', case=False, regex=False, na=False)]\n",
    "both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_path': '201901/01/20190101_164846_000.jpg',\n",
       " 'minute_id': '20190101_1648',\n",
       " 'time': '2019/01/01 16:48:00+0000',\n",
       " 'utc_time': '2019/01/01 16:48:00+0000',\n",
       " 'weekday': 'tuesday',\n",
       " 'descriptions': ['person',\n",
       "  'food',\n",
       "  'table',\n",
       "  'plate',\n",
       "  'indoor',\n",
       "  'eating',\n",
       "  'dessert',\n",
       "  'meal'],\n",
       " 'address': 'Dublin, Ireland, Leinster',\n",
       " 'location': \"Eddie Rocket's\",\n",
       " 'gps': {'lat': 53.2828644, 'lon': -6.4222863},\n",
       " 'region': ['dublin', 'ireland', 'leinster'],\n",
       " 'country': 'ireland',\n",
       " 'ocr': '',\n",
       " 'timestamp': 1546361280.0,\n",
       " 'ocr_score': {},\n",
       " 'scene': 'S_253',\n",
       " 'group': 'G_17',\n",
       " 'before': ['201901/01/20190101_154407_000.jpg',\n",
       "  '201901/01/20190101_154439_000.jpg',\n",
       "  '201901/01/20190101_154511_000.jpg',\n",
       "  '201901/01/20190101_154543_000.jpg',\n",
       "  '201901/01/20190101_154615_000.jpg',\n",
       "  '201901/01/20190101_154647_000.jpg',\n",
       "  '201901/01/20190101_154719_000.jpg',\n",
       "  '201901/01/20190101_154751_000.jpg',\n",
       "  '201901/01/20190101_154823_000.jpg',\n",
       "  '201901/01/20190101_154855_000.jpg'],\n",
       " 'after': ['201901/01/20190101_170843_000.jpg',\n",
       "  '201901/01/20190101_170915_000.jpg',\n",
       "  '201901/01/20190101_170947_000.jpg',\n",
       "  '201901/01/20190101_171019_000.jpg',\n",
       "  '201901/01/20190101_171123_000.jpg',\n",
       "  '201901/01/20190101_171401_000.jpg',\n",
       "  '201901/01/20190101_171433_000.jpg',\n",
       "  '201901/01/20190101_171537_000.jpg',\n",
       "  '201901/01/20190101_171815_000.jpg']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_dict[\"201901/01/20190101_164846_000.jpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from unidecode import unidecode\n",
    "\n",
    "fields_to_fix = [\"address\", \"location\", \"region\"]\n",
    "for image in info_dict:\n",
    "    for field in fields_to_fix:\n",
    "        if isinstance(info_dict[image][field], str):\n",
    "            info_dict[image][field] = unidecode(\n",
    "                info_dict[image][field])\n",
    "        elif isinstance(info_dict[image][field], list):\n",
    "            info_dict[image][field] = [unidecode(s) for s in info_dict[image][field]]\n",
    "        elif np.isnan(info_dict[image][field]):\n",
    "            info_dict[image][field] = \"NONE\"\n",
    "        else:\n",
    "            print(field, info_dict[image][field])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(info_dict, open(f\"files/info_dict.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARE BACKEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779\n"
     ]
    }
   ],
   "source": [
    "locations = set([img[\"location\"].lower().strip() for img in info_dict.values()])\n",
    "if \"none\" in locations:\n",
    "    locations.remove(\"none\")\n",
    "extra = set()\n",
    "location_with_extras = {}\n",
    "for loc in locations:\n",
    "    location_with_extras[loc] = []\n",
    "    for lengram in range(2, len(loc)):\n",
    "        for ngram in ngrams(loc.split(), lengram):\n",
    "            location_with_extras[loc].append(\" \".join(ngram))\n",
    "    location_with_extras[loc].append(loc)\n",
    "    location_with_extras[loc] = location_with_extras[loc][::-1]\n",
    "json.dump(location_with_extras, open(f'files/backend/locations.json', 'w'))\n",
    "print(len(locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = set([loc.lower().strip() for img in info_dict.values()\n",
    "               for loc in img[\"region\"]])\n",
    "json.dump(list(regions), open(f'files/backend/regions.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"/home/tlduyen/LSC2020/LSC2020/ui/src/regions.js\", 'w') as f:\n",
    "    f.write(\"var regions=\" + json.dumps(list(regions)) + \";\\n\\nexport default regions;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords_counter = Counter([w for img in info_dict.values() for w in img[\"descriptions\"]])\n",
    "\n",
    "json.dump(all_keywords_counter, open(f'files/backend/all_keywords.json', 'w'))\n",
    "# all_keywords_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "for img in info_dict.values():\n",
    "    for w in img[\"descriptions\"]:\n",
    "        if w:\n",
    "            for w2 in img[\"descriptions\"]:\n",
    "                if w2 and w2!=w:\n",
    "                    overlap[w2][w] += 1\n",
    "\n",
    "json.dump(overlap, open(f'files/backend/overlap_keywords.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dict(image):\n",
    "    return { key: info_dict[image][key] for key in [\"group\", \"scene\", \"time\", \"gps\", \"location\"]}\n",
    "\n",
    "basic_dict = {image: filter_dict(image) for image in info_dict}\n",
    "json.dump(basic_dict, open(f'files/backend/basic_dict.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_info = {}\n",
    "def get_hour_minute(date_string):\n",
    "    datetime_value = datetime.strptime(date_string, \"%Y/%m/%d %H:%M:00%z\")\n",
    "    return datetime_value.strftime(\"%I:%M%p\")\n",
    "\n",
    "def get_final_time(first_info, last_info):\n",
    "    if first_info == last_info:\n",
    "        return first_info\n",
    "    return f\"{first_info} - {last_info}\"\n",
    "\n",
    "for group_name in groups:\n",
    "    group_first_info = None\n",
    "    group_last_info = None\n",
    "    for scene_name, images in groups[group_name][\"scenes\"]:\n",
    "        first_info = info_dict[images[0]][\"time\"]\n",
    "        last_info = info_dict[images[-1]][\"time\"]\n",
    "        if not group_first_info:\n",
    "            group_first_info = first_info\n",
    "        group_last_info = last_info\n",
    "        time_info[scene_name] = get_final_time(get_hour_minute(first_info), get_hour_minute(last_info))\n",
    "    time_info[group_name] = get_final_time(get_hour_minute(group_first_info), get_hour_minute(group_last_info))\n",
    "\n",
    "json.dump(time_info, open(f\"files/backend/time_info.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11:02AM'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_info[\"S_36\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "82043e9c6bb60dc85d822aa1bc58ee389a3953da33585b1a533ca4c796d4eb5e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
