{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import bamboolib as bam\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "import json\n",
    "import geopy.distance\n",
    "from nltk import ngrams\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_for_ocr(word):\n",
    "    final_text = defaultdict(float)\n",
    "    final_text[word] = 1\n",
    "    for i in range(0, len(word)-1):\n",
    "        if len(word[:i+1]) > 1:\n",
    "            final_text[word[:i+1]] += (i+1) / len(word)\n",
    "        if len(word[i+1:]) > 1:\n",
    "            final_text[word[i+1:]] += 1 - (i+1)/len(word)\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ocr_scores = json.load(open(\"../original_data/OCR_201901.json\"))\n",
    "ocr_scores = {}\n",
    "# for year in os.listdir(\"/home/nmduy/LSC2022/LSC_Metada/OCR/text_area/\"):\n",
    "#     if \".json\" in year:\n",
    "#         ocr_scores.update(json.load(open(f\"/home/nmduy/LSC2022/LSC_Metada/OCR/text_area/{year}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unhelpful_images = json.load(open(\"files/unhelpful_images.json\"))\n",
    "# metadata = pd.read_csv('files/processed.csv', sep=',', decimal='.')\n",
    "metadata = pd.read_csv('VAISL/files/final_metadata.csv', sep=',', decimal='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Old timezone processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Europe/Dublin          150918\n",
       "Asia/Bangkok             7504\n",
       "Europe/Athens            2042\n",
       "Asia/Ho_Chi_Minh         1763\n",
       "Europe/Madrid            1494\n",
       "Europe/Istanbul          1156\n",
       "Europe/London            1017\n",
       "Asia/Seoul                938\n",
       "Europe/Paris              588\n",
       "Europe/Berlin             502\n",
       "Europe/Zurich             399\n",
       "Australia/Melbourne       383\n",
       "Europe/Copenhagen         220\n",
       "Europe/Bucharest          131\n",
       "Europe/Oslo               127\n",
       "America/Toronto            83\n",
       "Etc/GMT-6                  67\n",
       "Etc/GMT                    62\n",
       "Europe/Sofia               55\n",
       "Etc/GMT-8                  36\n",
       "Europe/Belgrade            10\n",
       "Europe/Amsterdam            9\n",
       "Asia/Phnom_Penh             7\n",
       "Europe/Zagreb               6\n",
       "Asia/Kolkata                5\n",
       "Europe/Budapest             5\n",
       "Europe/Brussels             3\n",
       "Europe/Ljubljana            2\n",
       "Europe/Vienna               2\n",
       "Etc/GMT-2                   2\n",
       "Name: time_zone, dtype: Int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "metadata['time_zone'] = metadata['time_zone'].astype('string')\n",
    "metadata['time_zone'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 100\n",
    "timezones = metadata['time_zone'].value_counts()\n",
    "cutoff = len(timezones)\n",
    "for i in range(len(timezones)):\n",
    "    if timezones[i] < threshold:\n",
    "        cutoff = i\n",
    "        break\n",
    "okay_timezones = timezones.index.values[:cutoff].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_condition = metadata['time_zone'].isin(okay_timezones)\n",
    "metadata.loc[tmp_condition, 'time_zone'] = metadata['time_zone']\n",
    "metadata.loc[~tmp_condition, 'time_zone'] = np.nan\n",
    "metadata[['time_zone']] = metadata[['time_zone']].fillna(method='ffill')\n",
    "metadata[['time_zone']] = metadata[['time_zone']].fillna(method='bfill')\n",
    "metadata['ImageID'] = metadata['ImageID'].astype('string')\n",
    "# metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_zones = {\"Europe/Dublin\": \"ireland\",\n",
    "              \"Europe/Athens\": \"greece\",\n",
    "              \"Europe/Berlin\": \"germany\",\n",
    "              \"Asia/Bangkok\": \"thailand\",\n",
    "              \"Asia/Ho_Chi_Minh\": \"vietnam\",\n",
    "              \"Europe/Madrid\": \"spain\",\n",
    "              \"Europe/Istanbul\": \"turkey\",\n",
    "              \"Europe/London\": \"england\",\n",
    "              \"Asia/Seoul\": \"korea\",\n",
    "              \"Europe/Paris\": \"france\",\n",
    "              \"Europe/Zurich\": \"switzerland\",\n",
    "              \"Australia/Melbourne\": \"australia\",\n",
    "              \"Europe/Copenhagen\": \"denmark\",\n",
    "              \"Europe/Bucharest\": \"romania\",\n",
    "              \"Europe/Oslo\": \"norway\"}\n",
    "all_countries = time_zones.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New timezone processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geojson\n",
    "country_geojson = geojson.load(open(\"../original_data/countries.geojson\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_countries = set(metadata[\"country\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "geojson_data = {}\n",
    "for country in country_geojson[\"features\"]:\n",
    "    name = country[\"properties\"][\"ADMIN\"]\n",
    "    if name.lower() in all_countries or name in [\"United Kingdom\", \"South Korea\"]:\n",
    "        geojson_data[name] = country\n",
    "geojson_data[\"Korea\"] = geojson_data[\"South Korea\"]\n",
    "geojson_data[\"England\"] = geojson_data[\"United Kingdom\"]\n",
    "\n",
    "with open(f\"/home/tlduyen/LSC2020/LSC2020/ui/src/worldmap.js\", 'w') as f:\n",
    "    f.write(\"var worldmap=\" + json.dumps(geojson_data) + \";\\n\\nexport {worldmap};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(geojson_data, open(\"files/backend/countries.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# def create(ocr_scores):\n",
    "#     tf = {}\n",
    "#     idf = defaultdict(lambda: 0)\n",
    "#     for image, scores in tqdm(ocr_scores.items()):\n",
    "#         tf[image] = defaultdict(float)\n",
    "#         word_set = set()\n",
    "#         for score in scores:\n",
    "#             word = score['text'].lower()\n",
    "#             for subword in word.lower().split():\n",
    "#                 if len(subword) > 1:\n",
    "#                     splited_words = process_for_ocr(subword)\n",
    "#                     for w, s in splited_words.items():\n",
    "#                         if w not in stop_words:\n",
    "#                             word_set.add(w)\n",
    "#                             tf[image][w] += np.log(1 + score['area'] * 5000 * s)\n",
    "#         for word in word_set:\n",
    "#             idf[word] += 1\n",
    "\n",
    "#     tf_idf = {}\n",
    "#     print(len(tf))\n",
    "#     for image in tqdm(tf):\n",
    "#         tf_idf[image] = {}\n",
    "#         for word in tf[image]:\n",
    "#             if idf[word]:\n",
    "#                 tf_idf[image][word] = tf[image][word] * np.log(len(tf) / idf[word])\n",
    "#                 assert (tf_idf[image][word] >= 0), f\"negative value {tf_idf[image][word]}, {tf[image][word]}, {word}, {idf[word]}, {np.log(len(tf) / idf[word])}\"\n",
    "#             else:\n",
    "#                 tf_idf[image][word] = 0\n",
    "#     return idf, tf_idf\n",
    "\n",
    "# idf, tf_idf = create(ocr_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idf = dict(idf.items())\n",
    "# joblib.dump((tf_idf, idf), \"files/ocr_tfidf.joblib\")\n",
    "# tf_idf, idf = joblib.load(\"files/ocr_tfidf.joblib\")\n",
    "tf_idf, idf = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[\"new_timezone\"] = metadata[\"new_timezone\"].ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a9c409609a4ac893ea4af6145c3a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/723329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "info_dict = {}\n",
    "\n",
    "def to_local_time(utc_time, time_zone):\n",
    "    return utc_time.astimezone(tz.gettz(time_zone))\n",
    "\n",
    "def to_full_key(image):\n",
    "    return f\"{image[:6]}/{image[6:8]}/{image}\"\n",
    "\n",
    "for index, row in tqdm(metadata.iterrows(), total=len(metadata)):\n",
    "    image = row['ImageID']\n",
    "    if isinstance(image, str):\n",
    "        if image not in unhelpful_images:\n",
    "            utc_time = datetime.strptime(row[\"minute_id\"]+\"00\", \"%Y%m%d_%H%M%S\").replace(tzinfo=tz.gettz('UTC'))\n",
    "            local_time = to_local_time(utc_time, row[\"new_timezone\"])\n",
    "            #TODO!\n",
    "            image = to_full_key(image)\n",
    "            info_dict[image] = {\n",
    "                \"image_path\": image,\n",
    "                \"minute_id\": row[\"minute_id\"],\n",
    "                \"time\": datetime.strftime(local_time, \"%Y/%m/%d %H:%M:00%z\"),\n",
    "                \"utc_time\": datetime.strftime(utc_time, \"%Y/%m/%d %H:%M:00%z\"),\n",
    "                \"weekday\": datetime.strftime(local_time, \"%A\").lower(),\n",
    "                \"descriptions\": row['Tags'].lower().split(',') if isinstance(row['Tags'], str) else \"\",\n",
    "                \"address\": row[\"city\"],\n",
    "                \"location\": row[\"checkin\"] if row[\"stop\"] else \"None\",\n",
    "                \"location_info\": row[\"categories\"] if row[\"stop\"] else row[\"checkin\"],\n",
    "                \"gps\": {\"lat\": row[\"new_lat\"],\n",
    "                        \"lon\": row[\"new_lng\"]},\n",
    "                \"region\": row[\"city\"].lower().split(', ') if isinstance(row[\"city\"], str) else [],\n",
    "                \"country\": row[\"country\"].lower() if isinstance(row[\"country\"], str) else None,\n",
    "                \"ocr\": row[\"OCR\"].split(', ') if isinstance(row['OCR'], str) else \"\",\n",
    "                \"timestamp\": utc_time.timestamp() #!TODO in es.py\n",
    "            }\n",
    "\n",
    "            if image in tf_idf:\n",
    "                info_dict[image][\"ocr_score\"] = dict([item for item in tf_idf[image].items() if item[1] > 0])\n",
    "            else:\n",
    "                info_dict[image][\"ocr_score\"] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_path': '201902/08/20190208_172845_000.jpg',\n",
       " 'minute_id': '20190208_1728',\n",
       " 'time': '2019/02/08 20:28:00+0300',\n",
       " 'utc_time': '2019/02/08 17:28:00+0000',\n",
       " 'weekday': 'friday',\n",
       " 'descriptions': ['text', 'person', 'indoor', 'store'],\n",
       " 'address': 'Turkey, Marmara',\n",
       " 'location': 'Istanbul Ataturk Airport',\n",
       " 'location_info': 'Airport',\n",
       " 'gps': {'lat': 40.984292, 'lon': 28.8156077},\n",
       " 'region': ['turkey', 'marmara'],\n",
       " 'country': 'turkey',\n",
       " 'ocr': ['gặp,lại,ain,TRÀ,CHANH,CROS,KHUY,Chupa,Chúps,TMINT,DOUDLEMIN,ININTIN'],\n",
       " 'timestamp': 1549646880.0,\n",
       " 'ocr_score': {}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_dict[\"201902/08/20190208_172845_000.jpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714583 714583\n"
     ]
    }
   ],
   "source": [
    "groups = json.load(open('files/group_segments.json'))\n",
    "\n",
    "assigned = []\n",
    "count = 0\n",
    "for group_name in groups:\n",
    "    group_id = int(group_name.split('_')[-1])\n",
    "    before = []\n",
    "    if group_id > 1:\n",
    "        before_group_id = group_id - 1\n",
    "        while before_group_id >= 1:\n",
    "            before_group = groups[f\"G_{before_group_id}\"]\n",
    "            if before_group[\"location\"] != \"NONE\":\n",
    "                for scene in before_group[\"scenes\"]:\n",
    "                    before.extend(scene[1])\n",
    "                break\n",
    "            before_group_id -= 1\n",
    "            \n",
    "    after = []\n",
    "    if group_id < len(groups):\n",
    "        after_group_id = group_id + 1\n",
    "        while after_group_id  <= len(groups) - 1:\n",
    "            after_group = groups[f\"G_{after_group_id}\"]\n",
    "            if after_group[\"location\"] != \"NONE\":\n",
    "                for scene in after_group[\"scenes\"]:\n",
    "                    after.extend(scene[1])\n",
    "                break\n",
    "            after_group_id +=1\n",
    "    for scene_name, images in groups[group_name][\"scenes\"]:\n",
    "        for image in images:\n",
    "            if image in info_dict:\n",
    "                info_dict[image][\"scene\"] = scene_name\n",
    "                info_dict[image][\"group\"] = group_name\n",
    "                info_dict[image][\"before\"] = before[:10]\n",
    "                info_dict[image][\"after\"] = after[:10]\n",
    "                count += 1\n",
    "                assigned.append(image)\n",
    "            else:\n",
    "                print(\"Skipping\", image)\n",
    "\n",
    "print(len(set(assigned)), len(info_dict))\n",
    "if len(set(assigned)) < len(info_dict):\n",
    "    for i, img in enumerate(info_dict):\n",
    "        if img not in assigned:\n",
    "            print(info_dict[img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_path': '201901/01/20190101_164846_000.jpg',\n",
       " 'minute_id': '20190101_1648',\n",
       " 'time': '2019/01/01 16:48:00+0000',\n",
       " 'utc_time': '2019/01/01 16:48:00+0000',\n",
       " 'weekday': 'tuesday',\n",
       " 'descriptions': ['person',\n",
       "  'food',\n",
       "  'table',\n",
       "  'plate',\n",
       "  'indoor',\n",
       "  'eating',\n",
       "  'dessert',\n",
       "  'meal'],\n",
       " 'address': 'Dublin, Ireland, Leinster',\n",
       " 'location': \"Eddie Rocket's\",\n",
       " 'location_info': 'Burger Joint, Diner, Fast Food Restaurant',\n",
       " 'gps': {'lat': 53.2828644, 'lon': -6.4222863},\n",
       " 'region': ['dublin', 'ireland', 'leinster'],\n",
       " 'country': 'ireland',\n",
       " 'ocr': '',\n",
       " 'timestamp': 1546361280.0,\n",
       " 'ocr_score': {},\n",
       " 'scene': 'S_388',\n",
       " 'group': 'G_177',\n",
       " 'before': ['201901/01/20190101_162244_000.jpg'],\n",
       " 'after': ['201901/01/20190101_170843_000.jpg']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_dict[\"201901/01/20190101_164846_000.jpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from unidecode import unidecode\n",
    "\n",
    "fields_to_fix = [\"address\", \"location\", \"region\"]\n",
    "for image in info_dict:\n",
    "    for field in fields_to_fix:\n",
    "        if isinstance(info_dict[image][field], str):\n",
    "            info_dict[image][field] = unidecode(\n",
    "                info_dict[image][field])\n",
    "        elif isinstance(info_dict[image][field], list):\n",
    "            info_dict[image][field] = [unidecode(s) for s in info_dict[image][field]]\n",
    "        elif np.isnan(info_dict[image][field]):\n",
    "            info_dict[image][field] = \"NONE\"\n",
    "        else:\n",
    "            print(field, info_dict[image][field])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(info_dict, open(f\"files/info_dict.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARE BACKEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720\n"
     ]
    }
   ],
   "source": [
    "locations = set([img[\"location\"].lower().strip() for img in info_dict.values()])\n",
    "if \"none\" in locations:\n",
    "    locations.remove(\"none\")\n",
    "extra = set()\n",
    "location_with_extras = {}\n",
    "for loc in locations:\n",
    "    location_with_extras[loc] = []\n",
    "    for lengram in range(2, len(loc)):\n",
    "        for ngram in ngrams(loc.split(), lengram):\n",
    "            location_with_extras[loc].append(\" \".join(ngram))\n",
    "    location_with_extras[loc].append(loc)\n",
    "    location_with_extras[loc] = location_with_extras[loc][::-1]\n",
    "json.dump(location_with_extras, open(f'files/backend/locations.json', 'w'))\n",
    "print(len(locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33465cc069554a4ebdaf60b05ff81f2c"
      },
      "text/plain": [
       "        Unnamed: 0.1                  ImageID  Unnamed: 0  \\\n",
       "530210        530210  20200109_133958_000.jpg      530210   \n",
       "\n",
       "                                 Tags                    OCR  \\\n",
       "530210  person,indoor,people,drinking  GUINNESS,LONDOR,PRIDE   \n",
       "\n",
       "                           Caption  CaptionScore movement  movement_prob  \\\n",
       "530210  a group of people in a bar      0.534637   Inside       0.922749   \n",
       "\n",
       "        inside  ...    new_lat     new_lng       checkin  \\\n",
       "530210    True  ...  37.534917  126.992729  The Fountain   \n",
       "\n",
       "                      checkin_id         original_name  \\\n",
       "530210  56d97b84cd10833070c3a4a5  The Fountain (더 파운틴)   \n",
       "\n",
       "                    categories  parent                city      country  \\\n",
       "530210  Whisky Bar, Restaurant     NaN  Seoul, South Korea  South Korea   \n",
       "\n",
       "       new_timezone  \n",
       "530210   Asia/Seoul  \n",
       "\n",
       "[1 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metadata = metadata.loc[metadata['ImageID'].str.contains('20200109_133958_000', case=False, regex=False, na=False)]\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = set([loc.lower().strip() for img in info_dict.values()\n",
    "               for loc in img[\"region\"]])\n",
    "json.dump(list(regions), open(f'files/backend/regions.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"/home/tlduyen/LSC2020/LSC2020/ui/src/regions.js\", 'w') as f:\n",
    "    f.write(\"var regions=\" + json.dumps(list(regions)) + \";\\n\\nexport default regions;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords_counter = Counter([w for img in info_dict.values() for w in img[\"descriptions\"]])\n",
    "\n",
    "json.dump(all_keywords_counter, open(f'files/backend/all_keywords.json', 'w'))\n",
    "# all_keywords_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "for img in info_dict.values():\n",
    "    for w in img[\"descriptions\"]:\n",
    "        if w:\n",
    "            for w2 in img[\"descriptions\"]:\n",
    "                if w2 and w2!=w:\n",
    "                    overlap[w2][w] += 1\n",
    "\n",
    "json.dump(overlap, open(f'files/backend/overlap_keywords.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dict(image):\n",
    "    return { key: info_dict[image][key] for key in [\"group\", \"scene\", \"time\", \"gps\", \"location\"]}\n",
    "\n",
    "basic_dict = {image: filter_dict(image) for image in info_dict}\n",
    "json.dump(basic_dict, open(f'files/backend/basic_dict.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_info = {}\n",
    "def get_hour_minute(date_string):\n",
    "    datetime_value = datetime.strptime(date_string, \"%Y/%m/%d %H:%M:00%z\")\n",
    "    return datetime_value.strftime(\"%I:%M%p\")\n",
    "\n",
    "def get_final_time(first_info, last_info):\n",
    "    if first_info == last_info:\n",
    "        return first_info\n",
    "    return f\"{first_info} - {last_info}\"\n",
    "\n",
    "for group_name in groups:\n",
    "    group_first_info = None\n",
    "    group_last_info = None\n",
    "    for scene_name, images in groups[group_name][\"scenes\"]:\n",
    "        first_info = info_dict[images[0]][\"time\"]\n",
    "        last_info = info_dict[images[-1]][\"time\"]\n",
    "        if not group_first_info:\n",
    "            group_first_info = first_info\n",
    "        group_last_info = last_info\n",
    "        time_info[scene_name] = get_final_time(get_hour_minute(first_info), get_hour_minute(last_info))\n",
    "    time_info[group_name] = get_final_time(get_hour_minute(group_first_info), get_hour_minute(group_last_info))\n",
    "\n",
    "json.dump(time_info, open(f\"files/backend/time_info.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11:02AM'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_info[\"S_36\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
