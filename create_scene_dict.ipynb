{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "import json\n",
    "from nltk import ngrams\n",
    "import bamboolib as bam"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unhelpful_images = json.load(open(\"files/unhelpful_images.json\"))\n",
    "metadata = pd.read_csv('VAISL/files/final_metadata.csv', sep=',', decimal='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(metadata):\n",
    "    metadata = metadata[['ImageID', 'minute_id', 'OCR', 'stop', 'movement', 'new_lat', 'new_lng', 'checkin', 'city', 'country', 'parent', 'new_timezone', 'Tags', 'categories']]\n",
    "    metadata['checkin'] = metadata['checkin'].fillna(\"\")\n",
    "    metadata['city'] = metadata['city'].fillna(\"\")\n",
    "    metadata.loc[metadata['new_timezone'] == 'uninhabited', 'new_timezone'] = \"\"\n",
    "    metadata['new_timezone'] = metadata['new_timezone'].ffill()\n",
    "    metadata[\"country\"] = metadata[\"country\"].fillna(\"\")\n",
    "    metadata[\"OCR\"] = metadata[\"OCR\"].fillna(\"\")\n",
    "    metadata[\"location_info\"] = metadata.apply(lambda row: row[\"categories\"] if row[\"stop\"] else \"\", axis=1)\n",
    "    metadata[\"location_info\"] = metadata[\"location_info\"].fillna(\"\")\n",
    "    return metadata\n",
    "metadata = preprocess(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "words_to_remove = [\"District\", \"Province\", \"Área metropolitana de Madrid y Corredor del Henares\", \"Community of\", \"The Municipal District of\",\n",
    "                  \"Kreis\", \"Landkreis\", \"Regional Unit\", \"Municipal Unit\", \"Municipality\", \"Administrative District\", \"Region of\",\n",
    "                  \"Provence-Alpes-Côte d'Azur\", \"Municipal Borough District\", \"Subdistrict Administrative Organization\", \"Subdistrict\",\n",
    "                  \"District\", \"Distretto di\", \"Municipal District\", \"City\", \"Land \", \"Urban agglomeration\"]\n",
    "words_to_remove = sorted(words_to_remove, key=lambda x: -len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_extra(city):\n",
    "    city = city.split(\",\")\n",
    "    new_city = []\n",
    "    for name in city:\n",
    "        for word in words_to_remove:\n",
    "            name = name.replace(word, \"\")\n",
    "        name = name.replace(\"of \", \"\")\n",
    "        name = name.strip()\n",
    "        if name and name not in new_city:\n",
    "            new_city.append(name)\n",
    "    return \", \".join(new_city)\n",
    "        \n",
    "metadata[\"city\"] = metadata[\"city\"].apply(remove_extra)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New timezone processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import geojson\n",
    "country_geojson = geojson.load(open(\"files/countries.geojson\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_countries = set(metadata[\"country\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "geojson_data = {}\n",
    "for country in country_geojson[\"features\"]:\n",
    "    name = country[\"properties\"][\"ADMIN\"]\n",
    "    if name in all_countries or name in [\"United Kingdom\", \"South Korea\"]:\n",
    "        geojson_data[name] = country\n",
    "geojson_data[\"Korea\"] = geojson_data[\"South Korea\"]\n",
    "geojson_data[\"England\"] = geojson_data[\"United Kingdom\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.dump(geojson_data, open(\"files/backend/countries.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34fa25fe2ef47d898330bb390be5db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/723329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "info_dict = {}\n",
    "def to_full_key(image):\n",
    "    return f\"{image[:6]}/{image[6:8]}/{image}\"\n",
    "\n",
    "def to_local_time(utc_time, time_zone):\n",
    "    return utc_time.astimezone(tz.gettz(time_zone))\n",
    "\n",
    "# Calculate seconds from midnight from a datetime object\n",
    "def seconds_from_midnight(time):\n",
    "    return time.hour * 3600 + time.minute * 60 + time.second\n",
    "\n",
    "for index, row in tqdm(metadata.iterrows(), total=len(metadata)):\n",
    "    image = row['ImageID']\n",
    "    if isinstance(image, str):\n",
    "        if image not in unhelpful_images:\n",
    "            image = to_full_key(image)\n",
    "            utc_time = datetime.strptime(row[\"minute_id\"]+\"00\", \"%Y%m%d_%H%M%S\").replace(tzinfo=tz.gettz('UTC'))\n",
    "            local_time = to_local_time(utc_time, row[\"new_timezone\"])\n",
    "            info_dict[image] = {\n",
    "                \"image_path\": image,\n",
    "                \"minute_id\": row[\"minute_id\"],\n",
    "                \"time\": datetime.strftime(local_time, \"%Y/%m/%d %H:%M:00%z\"),\n",
    "                \"utc_time\": datetime.strftime(utc_time, \"%Y/%m/%d %H:%M:00%z\"),\n",
    "                \"weekday\": datetime.strftime(local_time, \"%A\").lower(),\n",
    "                \"descriptions\": row['Tags'].lower().split(',') if isinstance(row['Tags'], str) else \"\",\n",
    "                \"address\": row[\"city\"],\n",
    "                \"location\": row[\"checkin\"] if row[\"stop\"] else \"---\",\n",
    "                \"location_info\": row[\"location_info\"],\n",
    "                \"gps\": {\"lat\": row[\"new_lat\"],\n",
    "                        \"lon\": row[\"new_lng\"]},\n",
    "                \"region\": row[\"city\"].lower().split(', '),\n",
    "                \"country\": row[\"country\"].lower(),\n",
    "                \"ocr\": str(row[\"OCR\"]).split(','),\n",
    "                \"timestamp\": utc_time.timestamp(),\n",
    "                \"seconds_from_midnight\": seconds_from_midnight(local_time)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "fields_to_fix = [\"address\", \"location\", \"region\", \"location_info\"]\n",
    "for image in info_dict:\n",
    "    for field in fields_to_fix:\n",
    "        if isinstance(info_dict[image][field], str):\n",
    "            info_dict[image][field] = unidecode(\n",
    "                info_dict[image][field])\n",
    "        elif isinstance(info_dict[image][field], list):\n",
    "            info_dict[image][field] = [unidecode(s) for s in info_dict[image][field]]\n",
    "        elif np.isnan(info_dict[image][field]):\n",
    "            info_dict[image][field] = \"NONE\"\n",
    "        else:\n",
    "            print(field, info_dict[image][field])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.dump(info_dict, open(\"files/info_dict.json\", \"w\"))\n",
    "# import json \n",
    "# info_dict = json.load(open(f\"files/info_dict.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_path': '202003/01/20200301_081459_000.jpg',\n",
       " 'minute_id': '20200301_0814',\n",
       " 'time': '2020/03/01 08:14:00+0000',\n",
       " 'utc_time': '2020/03/01 08:14:00+0000',\n",
       " 'weekday': 'sunday',\n",
       " 'descriptions': ['text',\n",
       "  'outdoor',\n",
       "  'road',\n",
       "  'sky',\n",
       "  'tree',\n",
       "  'street',\n",
       "  'way',\n",
       "  'highway',\n",
       "  'car'],\n",
       " 'address': 'Dublin, Ireland, Leinster',\n",
       " 'location': '---',\n",
       " 'location_info': '',\n",
       " 'gps': {'lat': 53.37971829369007, 'lon': -6.174530699771785},\n",
       " 'region': ['dublin', 'ireland', 'leinster'],\n",
       " 'country': 'ireland',\n",
       " 'ocr': ['09-D-27845', 'ELLL'],\n",
       " 'timestamp': 1583050440.0,\n",
       " 'seconds_from_midnight': 29640}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_dict[\"202003/01/20200301_081459_000.jpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "713861 713861\n"
     ]
    }
   ],
   "source": [
    "groups = json.load(open('files/group_segments.json'))\n",
    "scene_info = {}\n",
    "\n",
    "assigned = []\n",
    "count = 0\n",
    "for group_name in groups:\n",
    "    group_id = int(group_name.split('_')[-1])\n",
    "    valid_scenes = []\n",
    "    for scene_name, images in groups[group_name][\"scenes\"]:\n",
    "        images = [image for image in images if image in info_dict]\n",
    "        if not images:\n",
    "            continue\n",
    "        valid_scenes.append(scene_name)\n",
    "        scene_info[scene_name] = {\n",
    "            \"group\": group_name,\n",
    "            \"images\": images,\n",
    "            \"start_time\": info_dict[images[0]][\"time\"],\n",
    "            \"end_time\": info_dict[images[-1]][\"time\"],\n",
    "            \"start_timestamp\": info_dict[images[0]][\"timestamp\"],\n",
    "            \"end_timestamp\": info_dict[images[-1]][\"timestamp\"],\n",
    "            \"start_seconds_from_midnight\": info_dict[images[0]][\"seconds_from_midnight\"],\n",
    "            \"end_seconds_from_midnight\": info_dict[images[-1]][\"seconds_from_midnight\"],\n",
    "            \"duration\": info_dict[images[-1]][\"seconds_from_midnight\"] - info_dict[images[0]][\"seconds_from_midnight\"] + 1,\n",
    "        }\n",
    "        for key in [\"location\", \"location_info\", \"region\", \"country\", \"weekday\"]:\n",
    "            scene_info[scene_name][key] = info_dict[images[0]][key]\n",
    "        \n",
    "        for key in [\"gps\"]:\n",
    "            scene_info[scene_name][key] = [info_dict[image][key] for image in images]\n",
    "        \n",
    "        for key in [\"ocr\"]:\n",
    "            merged = Counter()\n",
    "            for image in images:\n",
    "                for text in info_dict[image][key]:\n",
    "                    if text not in merged:\n",
    "                        merged[text] += 1\n",
    "            scene_info[scene_name][key] = [a for a, _ in merged.most_common(10)]\n",
    "        \n",
    "        for image in images:\n",
    "            info_dict[image][\"scene\"] = scene_name\n",
    "            info_dict[image][\"group\"] = group_name\n",
    "            count += 1\n",
    "            assigned.append(image)\n",
    "    group_duration = scene_info[valid_scenes[-1]][\"end_seconds_from_midnight\"] - scene_info[valid_scenes[0]][\"start_seconds_from_midnight\"] + 1\n",
    "    for scene in valid_scenes:\n",
    "        scene_info[scene][\"group_duration\"] = group_duration\n",
    "\n",
    "print(len(set(assigned)), len(info_dict))\n",
    "# I THINK THERE'S SOMETHING WRONG HERE\n",
    "if len(set(assigned)) < len(info_dict):\n",
    "    to_remove = set(info_dict.keys()).difference(assigned)\n",
    "    for img in to_remove:\n",
    "        del info_dict[img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(info_dict, open(\"files/info_dict.json\", \"w\"))\n",
    "json.dump(scene_info, open(f\"files/scene_dict.json\", \"w\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARE BACKEND"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "719\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "def encode_query(main_query):\n",
    "    main_query = clip.tokenize([main_query]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_encoded = clip_model.encode_text(main_query)\n",
    "        text_encoded /= text_encoded.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_encoded.cpu().numpy()\n",
    "    return text_features\n",
    "\n",
    "locations = set([img[\"location\"].lower().strip() for img in info_dict.values()])\n",
    "if \"none\" in locations:\n",
    "    locations.remove(\"none\")\n",
    "    \n",
    "# TfidfVectorizer \n",
    "# CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "# set of documents\n",
    "train = [location for location in locations]\n",
    "# instantiate the vectorizer object\n",
    "tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english')\n",
    "# convert th documents into a matrix\n",
    "tfidf_wm = tfidfvectorizer.fit_transform(train)    \n",
    "\n",
    "# Find the most similar locations based on tfidf\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def get_similar_locations_tfidf(location, top=5):\n",
    "    # generate tf-idf for the given document\n",
    "    tfidf_vector = tfidfvectorizer.transform([location])\n",
    "    # find similar locations\n",
    "    cosine_similarities = cosine_similarity(tfidf_vector, tfidf_wm)\n",
    "    similar_indices = cosine_similarities.argsort().flatten()[-top:]\n",
    "    # sort the similar locations by similarity\n",
    "    similar_locations = sorted([(train[i], cosine_similarities[0,i]) for i in similar_indices], key=lambda x: -x[1])\n",
    "    # filter zero similarity\n",
    "    similar_locations = [similar_location[0] for similar_location in similar_locations if similar_location[1] > 0]\n",
    "    return similar_locations\n",
    "\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_ngram_freqdist(n):\n",
    "    # Get the words from the Brown Corpus\n",
    "    words = brown.words()\n",
    "\n",
    "    # Tokenize and filter stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "\n",
    "    # Generate n-grams\n",
    "    ngrams = list(nltk.ngrams(filtered_words, n))\n",
    "\n",
    "    # Compute frequency distribution of n-grams\n",
    "    freq_dist = FreqDist(ngrams)\n",
    "    return freq_dist\n",
    "\n",
    "ngrams_list = []\n",
    "for n in range(2, 5):\n",
    "    freq_dist = get_ngram_freqdist(n)\n",
    "    for word, frequency in freq_dist.most_common(500):\n",
    "        ngrams_list.append(word)\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import MWETokenizer\n",
    "tokenizer = MWETokenizer(ngrams_list)\n",
    "tokenizer.add_mwe(('chiang', 'mai'))\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "location_with_extras = {}\n",
    "from itertools import combinations\n",
    "locations = [location for location in locations]\n",
    "for loc in locations:\n",
    "    if loc:\n",
    "        n_grams = []\n",
    "        tokens = tokenizer.tokenize(loc.split())\n",
    "        for lengram in range(2, len(tokens)):\n",
    "            for ngram in combinations(tokens, lengram):\n",
    "                # Remove non-alpha words\n",
    "                ngram = [word for word in ngram if word.isalpha()]\n",
    "                if not ngram:\n",
    "                    continue\n",
    "                while ngram[-1] in stop_words:\n",
    "                    ngram = ngram[:-1]\n",
    "                    if not ngram:\n",
    "                        break\n",
    "                if not ngram:\n",
    "                    continue\n",
    "                while ngram[0] in stop_words:\n",
    "                    ngram = ngram[1:]\n",
    "                    if not ngram:\n",
    "                        break\n",
    "                if len(ngram) <= 1:\n",
    "                    continue\n",
    "                word = \" \".join(ngram)\n",
    "                if word and word not in n_grams:\n",
    "                    n_grams.append(word)\n",
    "        n_grams = n_grams[::-1]\n",
    "        new_n_grams = []\n",
    "        loc_features = encode_query(loc)\n",
    "        for word in n_grams:\n",
    "            word_features = encode_query(word)\n",
    "            if loc_features @ word_features.T > 0.8:\n",
    "                if loc in get_similar_locations_tfidf(word, top=5):\n",
    "                    new_n_grams.append(word)\n",
    "        location_with_extras[loc] = new_n_grams\n",
    "location_with_extras['porridge in front of phrommet shrine'] = ['phrommet shrine', 'phrommet']\n",
    "location_with_extras['on the matthews coach in transit'] = ['matthews coach']\n",
    "location_with_extras['the inn on the mile'] = ['inn on the mile']\n",
    "json.dump(location_with_extras, open(f'files/backend/locations.json', 'w'))\n",
    "print(len(locations))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "regions = set([loc.lower().strip() for img in info_dict.values()\n",
    "               for loc in img[\"region\"]])\n",
    "json.dump(list(regions), open(f'files/backend/regions.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f\"../UI/src/regions.js\", 'w') as f:\n",
    "    f.write(\"var regions=\" + json.dumps(list(regions)) + \";\\n\\nexport default regions;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "location_infos = Counter([loc for img in info_dict.values() for loc in img['location_info'].lower().strip().split(',')])\n",
    "location_infos = list(location_infos.keys())\n",
    "location_infos = [loc.strip() for loc in location_infos if loc]\n",
    "json.dump(list(location_infos), open(f'files/backend/location_info.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "info_dict = json.load(open(\"files/info_dict.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_dict(image):\n",
    "    return { key: info_dict[image][key] for key in [\"group\", \"scene\", \"time\", \"gps\", \n",
    "                                                    \"location\", \"location_info\", \"country\", \"ocr\"]}\n",
    "\n",
    "basic_dict = {image: filter_dict(image) for image in info_dict}\n",
    "json.dump(basic_dict, open(f'files/backend/basic_dict.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_info = {}\n",
    "def get_hour_minute(date_string):\n",
    "    datetime_value = datetime.strptime(date_string, \"%Y/%m/%d %H:%M:00%z\")\n",
    "    return datetime_value.strftime(\"%I:%M%p\")\n",
    "\n",
    "def get_final_time(first_info, last_info):\n",
    "    if first_info == last_info:\n",
    "        return first_info\n",
    "    return f\"{first_info} - {last_info}\"\n",
    "\n",
    "for group_name in groups:\n",
    "    group_first_info = None\n",
    "    group_last_info = None\n",
    "    for scene_name, images in groups[group_name][\"scenes\"]:\n",
    "        first_info = info_dict[images[0]][\"time\"]\n",
    "        last_info = info_dict[images[-1]][\"time\"]\n",
    "        if not group_first_info:\n",
    "            group_first_info = first_info\n",
    "        group_last_info = last_info\n",
    "        time_info[scene_name] = get_final_time(get_hour_minute(first_info), get_hour_minute(last_info))\n",
    "    time_info[group_name] = get_final_time(get_hour_minute(group_first_info), get_hour_minute(group_last_info))\n",
    "\n",
    "json.dump(time_info, open(f\"files/backend/time_info.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "b8b690c92a12f7605c72977ba1e7228395634b32ebd0b5cb3511f38cdc501d91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
