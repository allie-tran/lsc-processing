{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import bamboolib as bam\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "import json\n",
    "import geopy.distance\n",
    "from nltk import ngrams\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_scores = {}\n",
    "# for year in os.listdir(\"/home/nmduy/LSC2022/LSC_Metada/OCR/text_area/\"):\n",
    "#     if \".json\" in year:\n",
    "#         ocr_scores.update(json.load(open(f\"/home/nmduy/LSC2022/LSC_Metada/OCR/text_area/{year}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unhelpful_images = json.load(open(\"files/unhelpful_images.json\"))\n",
    "metadata = pd.read_csv('VAISL/files/final_metadata.csv', sep=',', decimal='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['checkin'] = metadata['checkin'].fillna(\"\")\n",
    "metadata['city'] = metadata['city'].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove = [\"Province\", \"Área metropolitana de Madrid y Corredor del Henares\", \"Community of\", \"The Municipal District of\",\n",
    "                  \"Kreis\", \"Landkreis\", \"Regional Unit\", \"Municipal Unit\", \"Municipality\", \"Administrative District\", \"Region of\",\n",
    "                  \"Provence-Alpes-Côte d'Azur\", \"Municipal Borough District\", \"Subdistrict Administrative Organization\", \"Subdistrict\",\n",
    "                  \"District\", \"Distretto di\", \"Municipal District\", \"City\", \"Land \", \"Urban agglomeration\"]\n",
    "words_to_remove = sorted(words_to_remove, key=lambda x: -len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra(city):\n",
    "    city = city.split(\",\")\n",
    "    new_city = []\n",
    "    for name in city:\n",
    "        for word in words_to_remove:\n",
    "            name = name.replace(word, \"\")\n",
    "        name = name.replace(\"of \", \"\")\n",
    "        name = name.strip()\n",
    "        if name and name not in new_city:\n",
    "            new_city.append(name)\n",
    "    return \", \".join(new_city)\n",
    "        \n",
    "metadata[\"city\"] = metadata[\"city\"].apply(remove_extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New timezone processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geojson\n",
    "country_geojson = geojson.load(open(\"../original_data/countries.geojson\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_countries = set(metadata[\"country\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "geojson_data = {}\n",
    "for country in country_geojson[\"features\"]:\n",
    "    name = country[\"properties\"][\"ADMIN\"]\n",
    "    if name.lower() in all_countries or name in [\"United Kingdom\", \"South Korea\"]:\n",
    "        geojson_data[name] = country\n",
    "geojson_data[\"Korea\"] = geojson_data[\"South Korea\"]\n",
    "geojson_data[\"England\"] = geojson_data[\"United Kingdom\"]\n",
    "\n",
    "with open(f\"/home/tlduyen/LSC22/lsc2020-UI/src/worldmap.js\", 'w') as f:\n",
    "    f.write(\"var worldmap=\" + json.dumps(geojson_data) + \";\\n\\nexport {worldmap};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(geojson_data, open(\"files/backend/countries.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# from tqdm.auto import tqdm\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# def process_for_ocr(word):\n",
    "#     final_text = defaultdict(float)\n",
    "#     final_text[word] = 1\n",
    "#     for i in range(0, len(word)-1):\n",
    "#         if len(word[:i+1]) > 1:\n",
    "#             final_text[word[:i+1]] += (i+1) / len(word)\n",
    "#         if len(word[i+1:]) > 1:\n",
    "#             final_text[word[i+1:]] += 1 - (i+1)/len(word)\n",
    "#     return final_text\n",
    "\n",
    "# def create(ocr_scores):\n",
    "#     tf = {}\n",
    "#     idf = defaultdict(lambda: 0)\n",
    "#     for image, scores in tqdm(ocr_scores.items()):\n",
    "#         tf[image] = defaultdict(float)\n",
    "#         word_set = set()\n",
    "#         for score in scores:\n",
    "#             word = score['text'].lower()\n",
    "#             for subword in word.lower().split():\n",
    "#                 if len(subword) > 1:\n",
    "#                     splited_words = process_for_ocr(subword)\n",
    "#                     for w, s in splited_words.items():\n",
    "#                         if w not in stop_words:\n",
    "#                             word_set.add(w)\n",
    "#                             tf[image][w] += np.log(1 + score['area'] * 5000 * s)\n",
    "#         for word in word_set:\n",
    "#             idf[word] += 1\n",
    "\n",
    "#     tf_idf = {}\n",
    "#     print(len(tf))\n",
    "#     for image in tqdm(tf):\n",
    "#         tf_idf[image] = {}\n",
    "#         for word in tf[image]:\n",
    "#             if idf[word]:\n",
    "#                 tf_idf[image][word] = tf[image][word] * np.log(len(tf) / idf[word])\n",
    "#                 assert (tf_idf[image][word] >= 0), f\"negative value {tf_idf[image][word]}, {tf[image][word]}, {word}, {idf[word]}, {np.log(len(tf) / idf[word])}\"\n",
    "#             else:\n",
    "#                 tf_idf[image][word] = 0\n",
    "#     return idf, tf_idf\n",
    "\n",
    "# idf, tf_idf = create(ocr_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idf = dict(idf.items())\n",
    "# joblib.dump((tf_idf, idf), \"files/ocr_tfidf.joblib\")\n",
    "# tf_idf, idf = joblib.load(\"files/ocr_tfidf.joblib\")\n",
    "tf_idf, idf = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[\"new_timezone\"] = metadata[\"new_timezone\"].ffill()\n",
    "metadata[\"country\"] = metadata[\"country\"].fillna(\"\")\n",
    "metadata[\"OCR\"] = metadata[\"OCR\"].fillna(\"\")\n",
    "metadata[\"location_info\"] = metadata.apply(lambda row: row[\"categories\"] if row[\"stop\"] else row[\"checkin\"], axis=1)\n",
    "metadata[\"location_info\"] = metadata[\"location_info\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4094c2380124e85aa377971350d83f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/723329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "info_dict = {}\n",
    "def to_full_key(image):\n",
    "    return f\"{image[:6]}/{image[6:8]}/{image}\"\n",
    "\n",
    "def to_local_time(utc_time, time_zone):\n",
    "    return utc_time.astimezone(tz.gettz(time_zone))\n",
    "\n",
    "for index, row in tqdm(metadata.iterrows(), total=len(metadata)):\n",
    "    image = row['ImageID']\n",
    "    if isinstance(image, str):\n",
    "        if image not in unhelpful_images:\n",
    "            image = to_full_key(image)\n",
    "            utc_time = datetime.strptime(row[\"minute_id\"]+\"00\", \"%Y%m%d_%H%M%S\").replace(tzinfo=tz.gettz('UTC'))\n",
    "            local_time = to_local_time(utc_time, row[\"new_timezone\"])\n",
    "            info_dict[image] = {\n",
    "                \"image_path\": image,\n",
    "                \"minute_id\": row[\"minute_id\"],\n",
    "                \"time\": datetime.strftime(local_time, \"%Y/%m/%d %H:%M:00%z\"),\n",
    "                \"utc_time\": datetime.strftime(utc_time, \"%Y/%m/%d %H:%M:00%z\"),\n",
    "                \"weekday\": datetime.strftime(local_time, \"%A\").lower(),\n",
    "                \"descriptions\": row['Tags'].lower().split(',') if isinstance(row['Tags'], str) else \"\",\n",
    "                \"address\": row[\"city\"],\n",
    "                \"location\": row[\"checkin\"] if row[\"stop\"] else \"None\",\n",
    "                \"location_info\": row[\"location_info\"],\n",
    "                \"gps\": {\"lat\": row[\"new_lat\"],\n",
    "                        \"lon\": row[\"new_lng\"]},\n",
    "                \"region\": row[\"city\"].lower().split(', '),\n",
    "                \"country\": row[\"country\"].lower(),\n",
    "                \"ocr\": row[\"OCR\"].split(', '),\n",
    "                \"timestamp\": utc_time.timestamp() #!TODO in es.py\n",
    "            }\n",
    "\n",
    "            if image in tf_idf:\n",
    "                info_dict[image][\"ocr_score\"] = dict([item for item in tf_idf[image].items() if item[1] > 0])\n",
    "            else:\n",
    "                info_dict[image][\"ocr_score\"] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from unidecode import unidecode\n",
    "\n",
    "fields_to_fix = [\"address\", \"location\", \"region\"]\n",
    "for image in info_dict:\n",
    "    for field in fields_to_fix:\n",
    "        if isinstance(info_dict[image][field], str):\n",
    "            info_dict[image][field] = unidecode(\n",
    "                info_dict[image][field])\n",
    "        elif isinstance(info_dict[image][field], list):\n",
    "            info_dict[image][field] = [unidecode(s) for s in info_dict[image][field]]\n",
    "        elif np.isnan(info_dict[image][field]):\n",
    "            info_dict[image][field] = \"NONE\"\n",
    "        else:\n",
    "            print(field, info_dict[image][field])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json \n",
    "# info_dict = json.load(open(f\"files/info_dict.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_path': '201901/06/20190106_171105_000.jpg',\n",
       " 'minute_id': '20190106_1711',\n",
       " 'time': '2019/01/06 19:11:00+0200',\n",
       " 'utc_time': '2019/01/06 17:11:00+0000',\n",
       " 'weekday': 'sunday',\n",
       " 'descriptions': ['table', 'food', 'plate', 'dish', 'meal'],\n",
       " 'address': 'Thessaloniki, Macedonia and Thrace, Greece',\n",
       " 'location': 'Eat Skaste',\n",
       " 'location_info': 'Souvlaki Shop',\n",
       " 'gps': {'lat': 40.6361591, 'lon': 22.9366191},\n",
       " 'region': ['thessaloniki', 'macedonia and thrace', 'greece'],\n",
       " 'country': 'greece',\n",
       " 'ocr': [''],\n",
       " 'timestamp': 1546794660.0,\n",
       " 'ocr_score': {}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_dict[\"201901/06/20190106_171105_000.jpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "713861 714583\n"
     ]
    }
   ],
   "source": [
    "groups = json.load(open('files/group_segments.json'))\n",
    "scene_info = {}\n",
    "\n",
    "assigned = []\n",
    "count = 0\n",
    "for group_name in groups:\n",
    "    group_id = int(group_name.split('_')[-1])\n",
    "    for scene_name, images in groups[group_name][\"scenes\"]:\n",
    "        images = [image for image in images if image in info_dict]\n",
    "        if not images:\n",
    "            continue\n",
    "        scene_info[scene_name] = {\n",
    "            \"group\": group_name,\n",
    "            \"images\": images,\n",
    "            \"start_time\": info_dict[images[0]][\"time\"],\n",
    "            \"end_time\": info_dict[images[-1]][\"time\"],\n",
    "            \"start_timestamp\": info_dict[images[0]][\"timestamp\"],\n",
    "            \"end_timestamp\": info_dict[images[-1]][\"timestamp\"]\n",
    "        }\n",
    "        for key in [\"location\", \"location_info\", \"region\", \"country\", \"weekday\"]:\n",
    "            scene_info[scene_name][key] = info_dict[images[0]][key]\n",
    "        \n",
    "        # for key in [\"gps\"]:\n",
    "            # scene_info[scene_name][key] = [info_dict[image][key] for image in images]\n",
    "        \n",
    "        for image in images:\n",
    "            info_dict[image][\"scene\"] = scene_name\n",
    "            info_dict[image][\"group\"] = group_name\n",
    "            count += 1\n",
    "            assigned.append(image)\n",
    "\n",
    "print(len(set(assigned)), len(info_dict))\n",
    "# I THINK THERE'S SOMETHING WRONG HERE\n",
    "if len(set(assigned)) < len(info_dict):\n",
    "    to_remove = set(info_dict.keys()).difference(assigned)\n",
    "    for img in to_remove:\n",
    "        del info_dict[img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'group': 'G_16',\n",
       " 'images': ['201901/01/20190101_164638_000.jpg',\n",
       "  '201901/01/20190101_164710_000.jpg',\n",
       "  '201901/01/20190101_164742_000.jpg',\n",
       "  '201901/01/20190101_164814_000.jpg',\n",
       "  '201901/01/20190101_164846_000.jpg',\n",
       "  '201901/01/20190101_164918_000.jpg',\n",
       "  '201901/01/20190101_164950_000.jpg',\n",
       "  '201901/01/20190101_165022_000.jpg',\n",
       "  '201901/01/20190101_165054_000.jpg',\n",
       "  '201901/01/20190101_165126_000.jpg',\n",
       "  '201901/01/20190101_165158_000.jpg'],\n",
       " 'start_time': '2019/01/01 16:46:00+0000',\n",
       " 'end_time': '2019/01/01 16:51:00+0000',\n",
       " 'start_timestamp': 1546361160.0,\n",
       " 'end_timestamp': 1546361460.0,\n",
       " 'location': \"Eddie Rocket's\",\n",
       " 'location_info': 'Burger Joint, Diner, Fast Food Restaurant',\n",
       " 'region': ['dublin', 'ireland', 'leinster'],\n",
       " 'country': 'ireland',\n",
       " 'weekday': 'tuesday'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_info[\"S_218\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.dump(info_dict, open(f\"files/info_dict.json\", \"w\"))\n",
    "json.dump(scene_info, open(f\"files/scene_dict.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARE BACKEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "718\n"
     ]
    }
   ],
   "source": [
    "locations = set([img[\"location\"].lower().strip() for img in info_dict.values()])\n",
    "if \"none\" in locations:\n",
    "    locations.remove(\"none\")\n",
    "extra = set()\n",
    "location_with_extras = {}\n",
    "for loc in locations:\n",
    "    if loc:\n",
    "        location_with_extras[loc] = []\n",
    "        for lengram in range(2, len(loc)):\n",
    "            for ngram in ngrams(loc.split(), lengram):\n",
    "                location_with_extras[loc].append(\" \".join(ngram))\n",
    "        location_with_extras[loc].append(loc)\n",
    "        location_with_extras[loc] = location_with_extras[loc][::-1]\n",
    "json.dump(location_with_extras, open(f'files/backend/locations.json', 'w'))\n",
    "print(len(locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"home\" in locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = set([loc.lower().strip() for img in info_dict.values()\n",
    "               for loc in img[\"region\"]])\n",
    "json.dump(list(regions), open(f'files/backend/regions.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"/home/tlduyen/LSC22/lsc2020-UI/src/regions.js\", 'w') as f:\n",
    "    f.write(\"var regions=\" + json.dumps(list(regions)) + \";\\n\\nexport default regions;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords_counter = Counter([w for img in info_dict.values() for w in img[\"descriptions\"]])\n",
    "\n",
    "json.dump(all_keywords_counter, open(f'files/backend/all_keywords.json', 'w'))\n",
    "# all_keywords_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dict(image):\n",
    "    return { key: info_dict[image][key] for key in [\"group\", \"scene\", \"time\", \"gps\", \"location\", \"location_info\"]}\n",
    "\n",
    "basic_dict = {image: filter_dict(image) for image in info_dict}\n",
    "json.dump(basic_dict, open(f'files/backend/basic_dict.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_info = {}\n",
    "def get_hour_minute(date_string):\n",
    "    datetime_value = datetime.strptime(date_string, \"%Y/%m/%d %H:%M:00%z\")\n",
    "    return datetime_value.strftime(\"%I:%M%p\")\n",
    "\n",
    "def get_final_time(first_info, last_info):\n",
    "    if first_info == last_info:\n",
    "        return first_info\n",
    "    return f\"{first_info} - {last_info}\"\n",
    "\n",
    "for group_name in groups:\n",
    "    group_first_info = None\n",
    "    group_last_info = None\n",
    "    for scene_name, images in groups[group_name][\"scenes\"]:\n",
    "        first_info = info_dict[images[0]][\"time\"]\n",
    "        last_info = info_dict[images[-1]][\"time\"]\n",
    "        if not group_first_info:\n",
    "            group_first_info = first_info\n",
    "        group_last_info = last_info\n",
    "        time_info[scene_name] = get_final_time(get_hour_minute(first_info), get_hour_minute(last_info))\n",
    "    time_info[group_name] = get_final_time(get_hour_minute(group_first_info), get_hour_minute(group_last_info))\n",
    "\n",
    "json.dump(time_info, open(f\"files/backend/time_info.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11:05AM - 11:11AM'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_info[\"S_36\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bamboolib",
   "language": "python",
   "name": "bamboolib"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "7896e60e377c4ef1762347c4784c3797065a53e3efc743d00c46d9f55f382c9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
